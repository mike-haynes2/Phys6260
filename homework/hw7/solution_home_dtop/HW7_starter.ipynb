{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7: Machine Learning in Physics\n",
    "## Your Name goes here\n",
    "Here we will be training a neural network to solve Burger's Equation that is a convective-diffusion PDE that appears in various physical systems.  It reads as\n",
    "\\begin{equation}\n",
    "\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2} = 0,\n",
    "\\quad\n",
    "x \\in [-1, 1],\n",
    "\\quad\n",
    "t \\in [0, 1]\n",
    "\\end{equation}\n",
    "\n",
    "* **Initial conditions:** $u(0,x) = -\\sin(\\pi x)$\n",
    "* Periodic boundary conditions\n",
    "\n",
    "*Note:* I used [black](https://github.com/psf/black) to auto-format my code, so some of its decisions are wonky but at least it's consistent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class to hold the model data, i.e. training and/or testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelData(object):\n",
    "    \"\"\"\n",
    "    Input/output data for the neural network because the example NN code shown in class had the same structure\n",
    "    Usage: obj = ModelData(x,y)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver for the Burger Equation\n",
    "Fill in the missing code in the PDE solver to use the FTUS (forward-time upwind-scheme) method.  We used this method in HW4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BurgerSolver(object):\n",
    "    def __init__(self, n=1000, nu=0.1, tend=1, cfl=0.01):\n",
    "        \"\"\"\n",
    "        Initialization of all parameters, solution arrays, and set the initial conditions\n",
    "        Inputs: n :: Number of x points\n",
    "                nu :: diffusion coefficient\n",
    "                tend :: stop time\n",
    "                cfl :: Courant factor controlling the timestep\n",
    "        \"\"\"\n",
    "        self.L = 2.0\n",
    "        self.n = n\n",
    "        self.dx = self.L / self.n\n",
    "        self.x = np.linspace(0, self.L, self.n) - self.L / 2  # Centered on zero\n",
    "        self.nu = nu\n",
    "        self.t = 0.0\n",
    "        self.tend = tend\n",
    "        self.dt = cfl * self.dx\n",
    "        print(self.dt)\n",
    "        self.Nt = int(self.tend / self.dt)\n",
    "        self.u = np.empty((self.n, self.Nt + 1))\n",
    "        self.dudx = np.zeros((self.n, self.Nt + 1))\n",
    "        self.iter = 0\n",
    "        self.seed = None\n",
    "\n",
    "        # Initial conditions\n",
    "        self.u[:, 0] = -np.sin(np.pi * self.x)\n",
    "\n",
    "    def plot_one(self, iter=None, show=True, save=False):\n",
    "        \"\"\"\n",
    "        Plot one solution at Iteration `iter`.  You can either `show` it or `save` it.\n",
    "        Default is to plot the final solution.\n",
    "        \"\"\"\n",
    "        if iter == None:\n",
    "            iter = self.iter  # last iteration\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(self.x, self.u[:, iter])\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"u\")\n",
    "        ax.set_title(f\"Time = {self.t:.4g} :: Iteration {iter}\")\n",
    "        if save:\n",
    "            plt.savefig(f\"burger-iter{iter:05d}.png\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def plot_evo(self, show=True, save=False):\n",
    "        \"\"\"\n",
    "        Plot all solutions as a space-time diagram.  You can either `show` it or `save` it.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        im = ax.imshow(\n",
    "            self.u,\n",
    "            origin=\"lower\",\n",
    "            cmap=\"RdBu\",\n",
    "            extent=[0, self.tend, -self.L / 2, self.L / 2],\n",
    "            aspect=\"auto\",\n",
    "        )\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"x\")\n",
    "        fig.colorbar(im, label=\"u\")\n",
    "        ax.set_title(f\"Burger's Equation evolution\")\n",
    "        if save:\n",
    "            plt.savefig(f\"burger-iter{self.iter:05d}.png\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def solve(self, pbar=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Numeric solver for the Burger equation.\n",
    "        \"\"\"\n",
    "        # Pre-compute some quantities\n",
    "        dx_inv = 1 / self.dx\n",
    "        dtdx = self.dt * dx_inv\n",
    "        dtdx2 = self.dt * dx_inv * dx_inv\n",
    "        du = np.empty(self.n)\n",
    "        if pbar and not verbose:\n",
    "            print(\n",
    "                \"0|\"\n",
    "                + \"-\" * 25\n",
    "                + \"|\"\n",
    "                + \"-\" * 25\n",
    "                + \"|\"\n",
    "                + \"-\" * 25\n",
    "                + \"|\"\n",
    "                + \"-\" * 25\n",
    "                + \"|100\"\n",
    "            )\n",
    "        while self.t < self.tend:\n",
    "            u = self.u[:, self.iter]  # Just for conciseness\n",
    "            unew = u.copy()\n",
    "\n",
    "            ### COMPLETE CODE HERE\n",
    "            # FTUS solver.\n",
    "            # self.u holds the solutions for all times.\n",
    "            # u is the current solution.\n",
    "            # unew is the future solution, which you need to compute with finite differencing.\n",
    "\n",
    "            self.u[:, self.iter + 1] = unew\n",
    "\n",
    "            # Use center differencing to compute du/dx from new solution.\n",
    "            # du/dx boundaries are already zero, so only modify [1:-1]\n",
    "            self.dudx[1:-1, self.iter + 1] = (unew[2:] - unew[:-2]) * dx_inv\n",
    "            self.iter += 1\n",
    "            self.t += self.dt\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Time = {self.t} seconds, mean/min/max = {unew.mean()} / {unew.min()} / {unew.max()}\"\n",
    "                )\n",
    "            elif pbar:\n",
    "                print(\"  \" + \"=\" * int(self.iter / self.Nt * 100) + \">\", end=\"\\r\")\n",
    "            if self.iter == self.Nt:\n",
    "                break\n",
    "\n",
    "    def random_sample(self, N, seed=None):\n",
    "        \"\"\"\n",
    "        Returns a ModelData object with random samples of u(x,t), x, t from our solution.\n",
    "        Used in this notebook to train a neural network below.\n",
    "        \"\"\"\n",
    "        if self.iter == 0:\n",
    "            print(\"Calculation not performed yet. Returning nothing.\")\n",
    "            return None\n",
    "        # Initialize RNG if not done already\n",
    "        if self.seed == None:\n",
    "            if seed != None:\n",
    "                self.seed = seed\n",
    "            else:\n",
    "                self.seed = int(time.time())\n",
    "            np.random.seed(self.seed)\n",
    "        randx = np.random.randint(1, self.n - 1, size=N)\n",
    "        randt = np.random.randint(0, self.Nt, size=N)\n",
    "        input = np.array([self.x[randx], self.dt * randt])\n",
    "        output = np.atleast_2d(self.u[randx, randt])\n",
    "        obj = ModelData(input, output)\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bs = BurgerSolver(tend=1, n=1000, nu=0.01, cfl=0.05)\n",
    "bs.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.plot_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.plot_evo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above space-time diagram is our dataset from which we will sample data points at random values of (x,t) that will be the inputs into our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network class for a single hidden layer\n",
    "You'll need to modify several parts of the code below, marked by `COMPLETE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    A neural network class with a single hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        solver,\n",
    "        num_training_unique=1000,\n",
    "        n_epochs=10,\n",
    "        learning_rate=0.1,\n",
    "        regularization_rate=0.05,\n",
    "        hidden_layer_size=2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialization routine for parameters, training data, and NN weights (matrices)\n",
    "        Inputs:\n",
    "        * solver :: PDE solver with a `random_sample` routine that returns a ModelData object.\n",
    "                    In HW7, it'd be a BurgerSolver object after you solve the system.\n",
    "        * num_training_unique :: Training dataset size\n",
    "        * n_epochs :: Number of training epochs, each randomly selecting `num_training_unique` inputs\n",
    "        * learning_rate :: Factor that softens the gradient descent during minimization\n",
    "        * regularization_rate :: Factor that controls how much the PDE residual is weighted (if used) in the loss function\n",
    "        * hidden_layer_size :: Number of nodes in the single hidden layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.solver = solver\n",
    "        self.num_training_unique = num_training_unique\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.train_set = self.solver.random_sample(self.num_training_unique)\n",
    "        self.normalize(set_limits=True)\n",
    "        self.set_boundary_points()\n",
    "        # learning rate\n",
    "        self.eta = learning_rate\n",
    "        self.lam = regularization_rate\n",
    "\n",
    "        # we get the size of the layers from the length of the input\n",
    "        # and output\n",
    "        model = self.train_set\n",
    "\n",
    "        # the number of nodes/neurons on the output layer\n",
    "        self.m = model.y.shape[0]\n",
    "\n",
    "        # the number of nodes/neurons on the input layer\n",
    "        self.n = model.x.shape[0]\n",
    "\n",
    "        # the number of nodes/neurons on the hidden layer\n",
    "        self.k = hidden_layer_size\n",
    "\n",
    "        # we will initialize the weights with Gaussian normal random\n",
    "        # numbers centered on 0 with a width of 1/sqrt(n), where n is\n",
    "        # the length of the input state\n",
    "\n",
    "        # A is the set of weights between the hidden layer and output layer\n",
    "        self.A = np.random.normal(0.0, 1.0 / np.sqrt(self.k), (self.m, self.k))\n",
    "\n",
    "        # B is the set of weights between the input layer and hidden layer\n",
    "        self.B = np.random.normal(0.0, 1.0 / np.sqrt(self.n), (self.k, self.n))\n",
    "\n",
    "    def normalize(self, model=None, set_limits=False):\n",
    "        \"\"\"\n",
    "        Training NNs typically work best when the inputs and outputs are normalized.\n",
    "        \"\"\"\n",
    "        if set_limits:\n",
    "            self.xmin = self.train_set.x.min(1)\n",
    "            self.xmax = self.train_set.x.max(1)\n",
    "            self.ymin = self.train_set.y.min()\n",
    "            self.ymax = self.train_set.y.max()\n",
    "        if model == None:\n",
    "            self.train_set.x = (self.train_set.x - self.xmin[:, None]) / (\n",
    "                self.xmax[:, None] - self.xmin[:, None]\n",
    "            )\n",
    "            self.train_set.y = (self.train_set.y - self.ymin) / (self.ymax - self.ymin)\n",
    "        else:\n",
    "            _model = copy.deepcopy(model)\n",
    "            _model.x = (_model.x - self.xmin[:, None]) / (\n",
    "                self.xmax[:, None] - self.xmin[:, None]\n",
    "            )\n",
    "            _model.y = (_model.y - self.ymin) / (self.ymax - self.ymin)\n",
    "            return _model\n",
    "\n",
    "    def denormalize(self, data):\n",
    "        \"\"\"\n",
    "        This function is used when we need to remove the normalization factors when providing predictions.\n",
    "        \"\"\"\n",
    "        data = data * (self.ymax - self.ymin) + self.ymin\n",
    "        return data\n",
    "\n",
    "    def set_activation(self, gtype=\"sigmoid\"):\n",
    "        if gtype not in [\"sigmoid\", \"relu\", \"tanh\", \"leaky_relu\"]:\n",
    "            raise RuntimeError(f\"Activation function {gtype} unknown.\")\n",
    "        self.gtype = gtype\n",
    "\n",
    "    def g(self, p, type=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        our activation function that operates on the hidden layer.\n",
    "        NOTE: AFAIK in this code, sigmoid is the only function that works in Burger's Equation for 1 hidden layer.\n",
    "        \"\"\"\n",
    "        if self.gtype == \"sigmoid\":\n",
    "            return 1.0 / (1.0 + np.exp(-p))\n",
    "        elif self.gtype == \"relu\":\n",
    "            return np.maximum(0, p)\n",
    "        elif self.gtype == \"leaky_relu\":\n",
    "            return np.maximum(0.1 * p, p)\n",
    "        elif self.gtype == \"tanh\":\n",
    "            return np.tanh(p)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Activation function {self.gtype} unknown.\")\n",
    "\n",
    "    def set_boundary_points(self, N=50):\n",
    "        \"\"\"\n",
    "        Sets points for the initial and boundary conditions so we can use them in the loss function.\n",
    "        \"\"\"\n",
    "        self.bc_x = np.linspace(self.xmin[0], self.xmax[0], N)\n",
    "        self.bc_t = np.linspace(self.xmin[1], self.xmax[1], N)\n",
    "\n",
    "        ###\n",
    "        ### COMPLETE HERE.  Set the analytic functions that describe the initial and boundary conditions.\n",
    "        ###\n",
    "        self.initial_fn = lambda x: None\n",
    "        self.boundary_fn = lambda x: None\n",
    "\n",
    "    def return_ic_loss(self):\n",
    "        \"\"\"\n",
    "        Returns the mean squared error between the predictions and analytic initial condition.\n",
    "        \"\"\"\n",
    "        ###\n",
    "        ### COMPLETE HERE.\n",
    "        ###\n",
    "        return 0.0\n",
    "\n",
    "    def return_bc_loss(self):\n",
    "        \"\"\"\n",
    "        Returns the mean squared error between the predictions and analytic boundary condition.\n",
    "        \"\"\"\n",
    "        ###\n",
    "        ### COMPLETE HERE.\n",
    "        ###\n",
    "        return 0.0\n",
    "\n",
    "    def return_deriv(self, x0, t0, dx=1e-3, dt=1e-3):\n",
    "        \"\"\"\n",
    "        Given a (x,t) value, this function retuns the following partial derivatives so we can compute the residual from Burger's Equation.\n",
    "        Inputs:\n",
    "        * x0 :: position to evaluate derivatives\n",
    "        * t0 :: time to evaluate derivatives\n",
    "        * dx :: distance between adjacent points when computing spatial derivatives\n",
    "        * dt :: time between adjacent points when computing time derivatives\n",
    "\n",
    "        Returns:\n",
    "        * u_t :: du/dt\n",
    "        * u_x :: du/dx\n",
    "        * u_xx :: d^2 u/dx^2\n",
    "        \"\"\"\n",
    "        ###\n",
    "        ### COMPLETE HERE\n",
    "        ###\n",
    "        # Use the NN to predict the u-values at nearby points and use central differencing to compute all the needed derivatives\n",
    "\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    def return_res_loss(self, u, x, t):\n",
    "        \"\"\"\n",
    "        Returns residual from Burger's equation: u_t + u*u_x - nu*u_xx = 0\n",
    "        \"\"\"\n",
    "        u_t, u_x, u_xx = self.return_deriv(x, t)\n",
    "        res = u_t + u * u_x - self.eta * u_xx\n",
    "        return np.atleast_2d(res)\n",
    "\n",
    "    def set_loss_function(self, fn=None, method=\"exact\"):\n",
    "        if fn is not None:\n",
    "            self.loss = fn\n",
    "        else:\n",
    "            if method == \"exact\":\n",
    "                self.loss = lambda x, y, z: z - y\n",
    "            # Add your methods here or assign a function\n",
    "            elif method == \"exact+initial\":\n",
    "                self.loss = lambda x, y, z: z - y + self.return_ic_loss()\n",
    "            elif method == \"exact+initial+boundary\":\n",
    "                self.loss = (\n",
    "                    lambda x, y, z: z - y\n",
    "                    + self.return_ic_loss()\n",
    "                    + self.return_bc_loss()\n",
    "                )\n",
    "            elif method == \"res+initial+boundary\":\n",
    "                self.loss = (\n",
    "                    lambda x, y, z: z - y\n",
    "                    + self.lam * self.return_res_loss(y[0], x[0], x[1])\n",
    "                    + self.return_ic_loss()\n",
    "                    + self.return_bc_loss()\n",
    "                )\n",
    "            else:\n",
    "                raise RuntimeError(f\"Method {method} not recognized.\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the neural network by doing gradient descent with back\n",
    "        propagation to set the matrix elements in B (the weights\n",
    "        between the input and hidden layer) and A (the weights between\n",
    "        the hidden layer and output layer)\n",
    "        \"\"\"\n",
    "        all_loss = []\n",
    "        print(\n",
    "            \"0|\" + \"-\" * 25 + \"|\" + \"-\" * 25 + \"|\" + \"-\" * 25 + \"|\" + \"-\" * 25 + \"|100\"\n",
    "        )\n",
    "        for iepoch in range(self.n_epochs):\n",
    "            # print(f\"epoch {i+1} of {self.n_epochs}\")\n",
    "            loss = 0.0\n",
    "            for _ in range(self.num_training_unique):\n",
    "                ii = np.random.randint(0, self.num_training_unique)\n",
    "\n",
    "                # Convert into 1D\n",
    "                x = self.train_set.x[:, ii].reshape(self.n, 1)\n",
    "                y = self.train_set.y[:, ii].reshape(self.m, 1)\n",
    "\n",
    "                z_tilde = self.g(self.B @ x)\n",
    "                z = self.g(self.A @ z_tilde)\n",
    "\n",
    "                e = self.loss(x, y, z)\n",
    "                loss += (e**2).sum()\n",
    "                if np.isinf(loss):\n",
    "                    raise RuntimeError(\n",
    "                        f\"Infinite loss function. Epoch {iepoch}, e = {e}, x,y = {x},{y}\"\n",
    "                    )\n",
    "                e_tilde = self.A.T @ e\n",
    "\n",
    "                dA = -2 * self.eta * e * z * (1 - z) @ z_tilde.T\n",
    "                dB = -2 * self.eta * e_tilde * z_tilde * (1 - z_tilde) @ x.T\n",
    "\n",
    "                self.A[:, :] += dA\n",
    "                self.B[:, :] += dB\n",
    "\n",
    "            print(\"  \" + \"=\" * int(iepoch / self.n_epochs * 100) + \">\", end=\"\\r\")\n",
    "            all_loss.append(loss)\n",
    "        return np.array(all_loss) / self.num_training_unique\n",
    "\n",
    "    def predict(self, model):\n",
    "        \"\"\"predict the outcome using our trained matrix A\"\"\"\n",
    "        nmodel = self.normalize(model=model)\n",
    "        y = self.g(self.A @ (self.g(self.B @ nmodel.x)))\n",
    "        return self.denormalize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(\n",
    "    bs, num_training_unique=2000, n_epochs=50, hidden_layer_size=40, learning_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.set_activation(\"sigmoid\")\n",
    "nn.set_loss_function(method=\"exact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
