{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7: Machine Learning in Physics\n",
    "### C. Michael Haynes; 04-04-25\n",
    "Here we will be training a neural network to solve Burger's Equation that is a convective-diffusion PDE that appears in various physical systems.  It reads as\n",
    "\\begin{equation}\n",
    "\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2} = 0,\n",
    "\\quad\n",
    "x \\in [-1, 1],\n",
    "\\quad\n",
    "t \\in [0, 1]\n",
    "\\end{equation}\n",
    "\n",
    "* **Initial conditions:** $u(0,x) = -\\sin(\\pi x)$\n",
    "* Periodic boundary conditions\n",
    "\n",
    "*Note:* I used [black](https://github.com/psf/black) to auto-format my code, so some of its decisions are wonky but at least it's consistent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import math as m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class to hold the model data, i.e. training and/or testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelData(object):\n",
    "    \"\"\"\n",
    "    Input/output data for the neural network because the example NN code shown in class had the same structure\n",
    "    Usage: obj = ModelData(x,y)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver for the Burger Equation\n",
    "Fill in the missing code in the PDE solver to use the FTUS (forward-time upwind-scheme) method.  We used this method in HW4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BurgerSolver(object):\n",
    "    def __init__(self, n=1000, nu=0.1, tend=1, cfl=0.01):\n",
    "        \"\"\"\n",
    "        Initialization of all parameters, solution arrays, and set the initial conditions\n",
    "        Inputs: n :: Number of x points\n",
    "                nu :: diffusion coefficient\n",
    "                tend :: stop time\n",
    "                cfl :: Courant factor controlling the timestep\n",
    "        \"\"\"\n",
    "        self.L = 2.0\n",
    "        self.n = n\n",
    "        self.dx = self.L / self.n\n",
    "        self.x = np.linspace(0, self.L, self.n) - self.L / 2  # Centered on zero\n",
    "        self.nu = nu\n",
    "        self.t = 0.0\n",
    "        self.tend = tend\n",
    "        self.dt = cfl * self.dx\n",
    "        print(self.dt)\n",
    "        self.Nt = int(self.tend / self.dt)\n",
    "        self.u = np.empty((self.n, self.Nt + 1))\n",
    "        self.dudx = np.zeros((self.n, self.Nt + 1))\n",
    "        self.iter = 0\n",
    "        self.seed = None\n",
    "\n",
    "        # Initial conditions\n",
    "        self.u[:, 0] = -np.sin(np.pi * self.x)\n",
    "\n",
    "\n",
    "    # FTCS function to evaluate each timestep\n",
    "    def advance_u(self, u):\n",
    "        # compute constants\n",
    "        dx_inv = 1. / self.dx\n",
    "        dtdx = self.dt * dx_inv\n",
    "        dtdx2 = self.dt * dx_inv * dx_inv\n",
    "        # calculate second term (since same for all points, doesn't care about sign of u)\n",
    "        alpha_arr = np.zeros_like(u)\n",
    "        beta_arr = np.zeros_like(u)\n",
    "        gamma_arr = np.zeros_like(u)\n",
    "        # second derivative term (same for all, since direction doesn't matter\n",
    "        beta_arr = self.nu * dtdx2 * (np.roll(u,1) + np.roll(u,-1) - 2.*u)\n",
    "        # first derivative term handling the upwindedness\n",
    "        for i,u_i in enumerate(u):\n",
    "            if u_i >= 0.:\n",
    "                alpha_arr[i] = u_i * (1. - dtdx * (u_i - u[i-1]))\n",
    "            elif u_i < 0:\n",
    "                alpha_arr[i] = u_i * (1. - dtdx * (u[i+1] - u_i))\n",
    "\n",
    "        gamma_arr = alpha_arr + beta_arr\n",
    "        return gamma_arr\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def plot_one(self, iter=None, show=True, save=False):\n",
    "        \"\"\"\n",
    "        Plot one solution at Iteration `iter`.  You can either `show` it or `save` it.\n",
    "        Default is to plot the final solution.\n",
    "        \"\"\"\n",
    "        if iter == None:\n",
    "            iter = self.iter  # last iteration\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(self.x, self.u[:, iter])\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"u\")\n",
    "        ax.set_title(f\"Time = {self.t:.4g} :: Iteration {iter}\")\n",
    "        if save:\n",
    "            plt.savefig(f\"burger-iter{iter:05d}.png\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def plot_evo(self, show=True, save=False):\n",
    "        \"\"\"\n",
    "        Plot all solutions as a space-time diagram.  You can either `show` it or `save` it.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        im = ax.imshow(\n",
    "            self.u,\n",
    "            origin=\"lower\",\n",
    "            cmap=\"RdBu\",\n",
    "            extent=[0, self.tend, -self.L / 2, self.L / 2],\n",
    "            aspect=\"auto\",\n",
    "        )\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"x\")\n",
    "        fig.colorbar(im, label=\"u\")\n",
    "        ax.set_title(f\"Burger's Equation evolution\")\n",
    "        if save:\n",
    "            plt.savefig(f\"burger-iter{self.iter:05d}.png\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def solve(self, pbar=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Numeric solver for the Burger equation.\n",
    "        \"\"\"\n",
    "        # Pre-compute some quantities\n",
    "        dx_inv = 1 / self.dx\n",
    "        dtdx = self.dt * dx_inv\n",
    "        dtdx2 = self.dt * dx_inv * dx_inv\n",
    "        du = np.empty(self.n)\n",
    "        if pbar and not verbose:\n",
    "            print(\n",
    "                \"0|\"\n",
    "                + \"-\" * 25\n",
    "                + \"|\"\n",
    "                + \"-\" * 25\n",
    "                + \"|\"\n",
    "                + \"-\" * 25\n",
    "                + \"|\"\n",
    "                + \"-\" * 25\n",
    "                + \"|100\"\n",
    "            )\n",
    "        while self.t < self.tend:\n",
    "            u = self.u[:, self.iter]  # Just for conciseness\n",
    "            unew = u.copy()\n",
    "\n",
    "            ### COMPLETE CODE HERE\n",
    "            unew = self.advance_u(u)\n",
    "            \n",
    "            # FTUS solver.\n",
    "            # self.u holds the solutions for all times.\n",
    "            # u is the current solution.\n",
    "            # unew is the future solution, which you need to compute with finite differencing.\n",
    "\n",
    "            self.u[:, self.iter + 1] = unew\n",
    "\n",
    "            # Use center differencing to compute du/dx from new solution.\n",
    "            # du/dx boundaries are already zero, so only modify [1:-1]\n",
    "            self.dudx[1:-1, self.iter + 1] = (unew[2:] - unew[:-2]) * dx_inv\n",
    "            self.iter += 1\n",
    "            self.t += self.dt\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Time = {self.t} seconds, mean/min/max = {unew.mean()} / {unew.min()} / {unew.max()}\"\n",
    "                )\n",
    "            elif pbar:\n",
    "                print(\"  \" + \"=\" * int(self.iter / self.Nt * 100) + \">\", end=\"\\r\")\n",
    "            if self.iter == self.Nt:\n",
    "                break\n",
    "\n",
    "    def random_sample(self, N, seed=None):\n",
    "        \"\"\"\n",
    "        Returns a ModelData object with random samples of u(x,t), x, t from our solution.\n",
    "        Used in this notebook to train a neural network below.\n",
    "        \"\"\"\n",
    "        if self.iter == 0:\n",
    "            print(\"Calculation not performed yet. Returning nothing.\")\n",
    "            return None\n",
    "        # Initialize RNG if not done already\n",
    "        if self.seed == None:\n",
    "            if seed != None:\n",
    "                self.seed = seed\n",
    "            else:\n",
    "                self.seed = int(time.time())\n",
    "            np.random.seed(self.seed)\n",
    "        randx = np.random.randint(1, self.n - 1, size=N)\n",
    "        randt = np.random.randint(0, self.Nt, size=N)\n",
    "        input = np.array([self.x[randx], self.dt * randt])\n",
    "        output = np.atleast_2d(self.u[randx, randt])\n",
    "        obj = ModelData(input, output)\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "0|-------------------------|-------------------------|-------------------------|-------------------------|100\n",
      "<__main__.BurgerSolver object at 0x7fbc48205e40> [ 1.22464680e-16  6.28943332e-03  1.25786178e-02  1.88673048e-02\n",
      "  2.51552454e-02  3.14421909e-02  3.77278927e-02  4.40121020e-02\n",
      "  5.02945704e-02  5.65750492e-02  6.28532900e-02  6.91290446e-02\n",
      "  7.54020646e-02  8.16721019e-02  8.79389084e-02  9.42022363e-02\n",
      "  1.00461838e-01  1.06717465e-01  1.12968871e-01  1.19215809e-01\n",
      "  1.25458030e-01  1.31695289e-01  1.37927338e-01  1.44153931e-01\n",
      "  1.50374822e-01  1.56589764e-01  1.62798512e-01  1.69000820e-01\n",
      "  1.75196443e-01  1.81385136e-01  1.87566653e-01  1.93740751e-01\n",
      "  1.99907185e-01  2.06065711e-01  2.12216086e-01  2.18358066e-01\n",
      "  2.24491409e-01  2.30615871e-01  2.36731210e-01  2.42837185e-01\n",
      "  2.48933554e-01  2.55020076e-01  2.61096510e-01  2.67162616e-01\n",
      "  2.73218154e-01  2.79262883e-01  2.85296566e-01  2.91318963e-01\n",
      "  2.97329837e-01  3.03328948e-01  3.09316061e-01  3.15290939e-01\n",
      "  3.21253344e-01  3.27203041e-01  3.33139795e-01  3.39063370e-01\n",
      "  3.44973534e-01  3.50870051e-01  3.56752688e-01  3.62621214e-01\n",
      "  3.68475395e-01  3.74315000e-01  3.80139798e-01  3.85949559e-01\n",
      "  3.91744053e-01  3.97523050e-01  4.03286322e-01  4.09033642e-01\n",
      "  4.14764781e-01  4.20479513e-01  4.26177612e-01  4.31858853e-01\n",
      "  4.37523010e-01  4.43169861e-01  4.48799180e-01  4.54410746e-01\n",
      "  4.60004337e-01  4.65579732e-01  4.71136709e-01  4.76675049e-01\n",
      "  4.82194534e-01  4.87694944e-01  4.93176062e-01  4.98637671e-01\n",
      "  5.04079556e-01  5.09501500e-01  5.14903290e-01  5.20284712e-01\n",
      "  5.25645553e-01  5.30985600e-01  5.36304643e-01  5.41602472e-01\n",
      "  5.46878875e-01  5.52133646e-01  5.57366576e-01  5.62577458e-01\n",
      "  5.67766086e-01  5.72932255e-01  5.78075760e-01  5.83196397e-01\n",
      "  5.88293965e-01  5.93368262e-01  5.98419086e-01  6.03446239e-01\n",
      "  6.08449521e-01  6.13428734e-01  6.18383682e-01  6.23314168e-01\n",
      "  6.28219997e-01  6.33100976e-01  6.37956911e-01  6.42787610e-01\n",
      "  6.47592882e-01  6.52372537e-01  6.57126385e-01  6.61854240e-01\n",
      "  6.66555913e-01  6.71231219e-01  6.75879973e-01  6.80501991e-01\n",
      "  6.85097090e-01  6.89665089e-01  6.94205806e-01  6.98719062e-01\n",
      "  7.03204679e-01  7.07662479e-01  7.12092285e-01  7.16493923e-01\n",
      "  7.20867219e-01  7.25211999e-01  7.29528091e-01  7.33815325e-01\n",
      "  7.38073532e-01  7.42302542e-01  7.46502188e-01  7.50672305e-01\n",
      "  7.54812728e-01  7.58923291e-01  7.63003834e-01  7.67054195e-01\n",
      "  7.71074213e-01  7.75063729e-01  7.79022586e-01  7.82950626e-01\n",
      "  7.86847695e-01  7.90713639e-01  7.94548304e-01  7.98351539e-01\n",
      "  8.02123193e-01  8.05863117e-01  8.09571163e-01  8.13247185e-01\n",
      "  8.16891037e-01  8.20502575e-01  8.24081656e-01  8.27628139e-01\n",
      "  8.31141882e-01  8.34622748e-01  8.38070599e-01  8.41485297e-01\n",
      "  8.44866709e-01  8.48214700e-01  8.51529138e-01  8.54809891e-01\n",
      "  8.58056831e-01  8.61269828e-01  8.64448755e-01  8.67593488e-01\n",
      "  8.70703900e-01  8.73779870e-01  8.76821275e-01  8.79827996e-01\n",
      "  8.82799913e-01  8.85736908e-01  8.88638867e-01  8.91505673e-01\n",
      "  8.94337213e-01  8.97133376e-01  8.99894051e-01  9.02619128e-01\n",
      "  9.05308500e-01  9.07962060e-01  9.10579704e-01  9.13161327e-01\n",
      "  9.15706829e-01  9.18216107e-01  9.20689063e-01  9.23125599e-01\n",
      "  9.25525619e-01  9.27889027e-01  9.30215731e-01  9.32505637e-01\n",
      "  9.34758657e-01  9.36974699e-01  9.39153678e-01  9.41295506e-01\n",
      "  9.43400098e-01  9.45467373e-01  9.47497247e-01  9.49489640e-01\n",
      "  9.51444475e-01  9.53361672e-01  9.55241158e-01  9.57082856e-01\n",
      "  9.58886695e-01  9.60652602e-01  9.62380509e-01  9.64070347e-01\n",
      "  9.65722048e-01  9.67335548e-01  9.68910783e-01  9.70447691e-01\n",
      "  9.71946209e-01  9.73406281e-01  9.74827847e-01  9.76210851e-01\n",
      "  9.77555239e-01  9.78860957e-01  9.80127955e-01  9.81356181e-01\n",
      "  9.82545587e-01  9.83696126e-01  9.84807753e-01  9.85880423e-01\n",
      "  9.86914095e-01  9.87908727e-01  9.88864280e-01  9.89780716e-01\n",
      "  9.90657999e-01  9.91496094e-01  9.92294968e-01  9.93054589e-01\n",
      "  9.93774928e-01  9.94455956e-01  9.95097645e-01  9.95699972e-01\n",
      "  9.96262911e-01  9.96786440e-01  9.97270539e-01  9.97715189e-01\n",
      "  9.98120372e-01  9.98486072e-01  9.98812274e-01  9.99098966e-01\n",
      "  9.99346136e-01  9.99553775e-01  9.99721874e-01  9.99850427e-01\n",
      "  9.99939428e-01  9.99988874e-01  9.99998764e-01  9.99969096e-01\n",
      "  9.99899872e-01  9.99791094e-01  9.99642768e-01  9.99454898e-01\n",
      "  9.99227492e-01  9.98960560e-01  9.98654111e-01  9.98308158e-01\n",
      "  9.97922715e-01  9.97497797e-01  9.97033420e-01  9.96529603e-01\n",
      "  9.95986366e-01  9.95403731e-01  9.94781719e-01  9.94120357e-01\n",
      "  9.93419671e-01  9.92679687e-01  9.91900435e-01  9.91081947e-01\n",
      "  9.90224253e-01  9.89327390e-01  9.88391391e-01  9.87416293e-01\n",
      "  9.86402137e-01  9.85348960e-01  9.84256806e-01  9.83125718e-01\n",
      "  9.81955739e-01  9.80746917e-01  9.79499299e-01  9.78212935e-01\n",
      "  9.76887875e-01  9.75524172e-01  9.74121880e-01  9.72681055e-01\n",
      "  9.71201752e-01  9.69684032e-01  9.68127953e-01  9.66533578e-01\n",
      "  9.64900969e-01  9.63230191e-01  9.61521310e-01  9.59774394e-01\n",
      "  9.57989512e-01  9.56166735e-01  9.54306134e-01  9.52407783e-01\n",
      "  9.50471757e-01  9.48498134e-01  9.46486990e-01  9.44438405e-01\n",
      "  9.42352462e-01  9.40229241e-01  9.38068827e-01  9.35871306e-01\n",
      "  9.33636764e-01  9.31365289e-01  9.29056973e-01  9.26711905e-01\n",
      "  9.24330180e-01  9.21911890e-01  9.19457131e-01  9.16966002e-01\n",
      "  9.14438599e-01  9.11875024e-01  9.09275378e-01  9.06639763e-01\n",
      "  9.03968283e-01  9.01261046e-01  8.98518156e-01  8.95739724e-01\n",
      "  8.92925858e-01  8.90076671e-01  8.87192274e-01  8.84272783e-01\n",
      "  8.81318312e-01  8.78328979e-01  8.75304901e-01  8.72246198e-01\n",
      "  8.69152992e-01  8.66025404e-01  8.62863558e-01  8.59667580e-01\n",
      "  8.56437596e-01  8.53173733e-01  8.49876121e-01  8.46544890e-01\n",
      "  8.43180172e-01  8.39782100e-01  8.36350809e-01  8.32886434e-01\n",
      "  8.29389112e-01  8.25858981e-01  8.22296182e-01  8.18700854e-01\n",
      "  8.15073141e-01  8.11413186e-01  8.07721134e-01  8.03997130e-01\n",
      "  8.00241323e-01  7.96453860e-01  7.92634891e-01  7.88784567e-01\n",
      "  7.84903042e-01  7.80990468e-01  7.77046999e-01  7.73072793e-01\n",
      "  7.69068007e-01  7.65032797e-01  7.60967326e-01  7.56871752e-01\n",
      "  7.52746238e-01  7.48590948e-01  7.44406046e-01  7.40191697e-01\n",
      "  7.35948067e-01  7.31675326e-01  7.27373642e-01  7.23043184e-01\n",
      "  7.18684125e-01  7.14296636e-01  7.09880892e-01  7.05437067e-01\n",
      "  7.00965337e-01  6.96465878e-01  6.91938869e-01  6.87384489e-01\n",
      "  6.82802917e-01  6.78194336e-01  6.73558927e-01  6.68896874e-01\n",
      "  6.64208361e-01  6.59493574e-01  6.54752698e-01  6.49985923e-01\n",
      "  6.45193436e-01  6.40375427e-01  6.35532086e-01  6.30663605e-01\n",
      "  6.25770177e-01  6.20851995e-01  6.15909254e-01  6.10942149e-01\n",
      "  6.05950876e-01  6.00935634e-01  5.95896621e-01  5.90834035e-01\n",
      "  5.85748078e-01  5.80638949e-01  5.75506853e-01  5.70351991e-01\n",
      "  5.65174567e-01  5.59974786e-01  5.54752854e-01  5.49508978e-01\n",
      "  5.44243365e-01  5.38956222e-01  5.33647760e-01  5.28318189e-01\n",
      "  5.22967718e-01  5.17596560e-01  5.12204928e-01  5.06793034e-01\n",
      "  5.01361093e-01  4.95909319e-01  4.90437928e-01  4.84947137e-01\n",
      "  4.79437162e-01  4.73908223e-01  4.68360536e-01  4.62794323e-01\n",
      "  4.57209803e-01  4.51607196e-01  4.45986726e-01  4.40348613e-01\n",
      "  4.34693081e-01  4.29020354e-01  4.23330656e-01  4.17624212e-01\n",
      "  4.11901248e-01  4.06161991e-01  4.00406666e-01  3.94635503e-01\n",
      "  3.88848729e-01  3.83046573e-01  3.77229264e-01  3.71397034e-01\n",
      "  3.65550112e-01  3.59688730e-01  3.53813119e-01  3.47923513e-01\n",
      "  3.42020143e-01  3.36103245e-01  3.30173050e-01  3.24229795e-01\n",
      "  3.18273715e-01  3.12305044e-01  3.06324020e-01  3.00330878e-01\n",
      "  2.94325855e-01  2.88309190e-01  2.82281120e-01  2.76241884e-01\n",
      "  2.70191721e-01  2.64130869e-01  2.58059569e-01  2.51978061e-01\n",
      "  2.45886586e-01  2.39785383e-01  2.33674696e-01  2.27554765e-01\n",
      "  2.21425832e-01  2.15288141e-01  2.09141933e-01  2.02987452e-01\n",
      "  1.96824941e-01  1.90654645e-01  1.84476807e-01  1.78291671e-01\n",
      "  1.72099483e-01  1.65900487e-01  1.59694928e-01  1.53483052e-01\n",
      "  1.47265105e-01  1.41041332e-01  1.34811980e-01  1.28577295e-01\n",
      "  1.22337524e-01  1.16092914e-01  1.09843712e-01  1.03590164e-01\n",
      "  9.73325184e-02  9.10710227e-02  8.48059245e-02  7.85374716e-02\n",
      "  7.22659119e-02  6.59914936e-02  5.97144649e-02  5.34350740e-02\n",
      "  4.71535694e-02  4.08701994e-02  3.45852128e-02  2.82988581e-02\n",
      "  2.20113839e-02  1.57230391e-02  9.43407223e-03  3.14473221e-03\n",
      " -3.14473221e-03 -9.43407223e-03 -1.57230391e-02 -2.20113839e-02\n",
      " -2.82988581e-02 -3.45852128e-02 -4.08701994e-02 -4.71535694e-02\n",
      " -5.34350740e-02 -5.97144649e-02 -6.59914936e-02 -7.22659119e-02\n",
      " -7.85374716e-02 -8.48059245e-02 -9.10710227e-02 -9.73325184e-02\n",
      " -1.03590164e-01 -1.09843712e-01 -1.16092914e-01 -1.22337524e-01\n",
      " -1.28577295e-01 -1.34811980e-01 -1.41041332e-01 -1.47265105e-01\n",
      " -1.53483052e-01 -1.59694928e-01 -1.65900487e-01 -1.72099483e-01\n",
      " -1.78291671e-01 -1.84476807e-01 -1.90654645e-01 -1.96824941e-01\n",
      " -2.02987452e-01 -2.09141933e-01 -2.15288141e-01 -2.21425832e-01\n",
      " -2.27554765e-01 -2.33674696e-01 -2.39785383e-01 -2.45886586e-01\n",
      " -2.51978061e-01 -2.58059569e-01 -2.64130869e-01 -2.70191721e-01\n",
      " -2.76241884e-01 -2.82281120e-01 -2.88309190e-01 -2.94325855e-01\n",
      " -3.00330878e-01 -3.06324020e-01 -3.12305044e-01 -3.18273715e-01\n",
      " -3.24229795e-01 -3.30173050e-01 -3.36103245e-01 -3.42020143e-01\n",
      " -3.47923513e-01 -3.53813119e-01 -3.59688730e-01 -3.65550112e-01\n",
      " -3.71397034e-01 -3.77229264e-01 -3.83046573e-01 -3.88848729e-01\n",
      " -3.94635503e-01 -4.00406666e-01 -4.06161991e-01 -4.11901248e-01\n",
      " -4.17624212e-01 -4.23330656e-01 -4.29020354e-01 -4.34693081e-01\n",
      " -4.40348613e-01 -4.45986726e-01 -4.51607196e-01 -4.57209803e-01\n",
      " -4.62794323e-01 -4.68360536e-01 -4.73908223e-01 -4.79437162e-01\n",
      " -4.84947137e-01 -4.90437928e-01 -4.95909319e-01 -5.01361093e-01\n",
      " -5.06793034e-01 -5.12204928e-01 -5.17596560e-01 -5.22967718e-01\n",
      " -5.28318189e-01 -5.33647760e-01 -5.38956222e-01 -5.44243365e-01\n",
      " -5.49508978e-01 -5.54752854e-01 -5.59974786e-01 -5.65174567e-01\n",
      " -5.70351991e-01 -5.75506853e-01 -5.80638949e-01 -5.85748078e-01\n",
      " -5.90834035e-01 -5.95896621e-01 -6.00935634e-01 -6.05950876e-01\n",
      " -6.10942149e-01 -6.15909254e-01 -6.20851995e-01 -6.25770177e-01\n",
      " -6.30663605e-01 -6.35532086e-01 -6.40375427e-01 -6.45193436e-01\n",
      " -6.49985923e-01 -6.54752698e-01 -6.59493574e-01 -6.64208361e-01\n",
      " -6.68896874e-01 -6.73558927e-01 -6.78194336e-01 -6.82802917e-01\n",
      " -6.87384489e-01 -6.91938869e-01 -6.96465878e-01 -7.00965337e-01\n",
      " -7.05437067e-01 -7.09880892e-01 -7.14296636e-01 -7.18684125e-01\n",
      " -7.23043184e-01 -7.27373642e-01 -7.31675326e-01 -7.35948067e-01\n",
      " -7.40191697e-01 -7.44406046e-01 -7.48590948e-01 -7.52746238e-01\n",
      " -7.56871752e-01 -7.60967326e-01 -7.65032797e-01 -7.69068007e-01\n",
      " -7.73072793e-01 -7.77046999e-01 -7.80990468e-01 -7.84903042e-01\n",
      " -7.88784567e-01 -7.92634891e-01 -7.96453860e-01 -8.00241323e-01\n",
      " -8.03997130e-01 -8.07721134e-01 -8.11413186e-01 -8.15073141e-01\n",
      " -8.18700854e-01 -8.22296182e-01 -8.25858981e-01 -8.29389112e-01\n",
      " -8.32886434e-01 -8.36350809e-01 -8.39782100e-01 -8.43180172e-01\n",
      " -8.46544890e-01 -8.49876121e-01 -8.53173733e-01 -8.56437596e-01\n",
      " -8.59667580e-01 -8.62863558e-01 -8.66025404e-01 -8.69152992e-01\n",
      " -8.72246198e-01 -8.75304901e-01 -8.78328979e-01 -8.81318312e-01\n",
      " -8.84272783e-01 -8.87192274e-01 -8.90076671e-01 -8.92925858e-01\n",
      " -8.95739724e-01 -8.98518156e-01 -9.01261046e-01 -9.03968283e-01\n",
      " -9.06639763e-01 -9.09275378e-01 -9.11875024e-01 -9.14438599e-01\n",
      " -9.16966002e-01 -9.19457131e-01 -9.21911890e-01 -9.24330180e-01\n",
      " -9.26711905e-01 -9.29056973e-01 -9.31365289e-01 -9.33636764e-01\n",
      " -9.35871306e-01 -9.38068827e-01 -9.40229241e-01 -9.42352462e-01\n",
      " -9.44438405e-01 -9.46486990e-01 -9.48498134e-01 -9.50471757e-01\n",
      " -9.52407783e-01 -9.54306134e-01 -9.56166735e-01 -9.57989512e-01\n",
      " -9.59774394e-01 -9.61521310e-01 -9.63230191e-01 -9.64900969e-01\n",
      " -9.66533578e-01 -9.68127953e-01 -9.69684032e-01 -9.71201752e-01\n",
      " -9.72681055e-01 -9.74121880e-01 -9.75524172e-01 -9.76887875e-01\n",
      " -9.78212935e-01 -9.79499299e-01 -9.80746917e-01 -9.81955739e-01\n",
      " -9.83125718e-01 -9.84256806e-01 -9.85348960e-01 -9.86402137e-01\n",
      " -9.87416293e-01 -9.88391391e-01 -9.89327390e-01 -9.90224253e-01\n",
      " -9.91081947e-01 -9.91900435e-01 -9.92679687e-01 -9.93419671e-01\n",
      " -9.94120357e-01 -9.94781719e-01 -9.95403731e-01 -9.95986366e-01\n",
      " -9.96529603e-01 -9.97033420e-01 -9.97497797e-01 -9.97922715e-01\n",
      " -9.98308158e-01 -9.98654111e-01 -9.98960560e-01 -9.99227492e-01\n",
      " -9.99454898e-01 -9.99642768e-01 -9.99791094e-01 -9.99899872e-01\n",
      " -9.99969096e-01 -9.99998764e-01 -9.99988874e-01 -9.99939428e-01\n",
      " -9.99850427e-01 -9.99721874e-01 -9.99553775e-01 -9.99346136e-01\n",
      " -9.99098966e-01 -9.98812274e-01 -9.98486072e-01 -9.98120372e-01\n",
      " -9.97715189e-01 -9.97270539e-01 -9.96786440e-01 -9.96262911e-01\n",
      " -9.95699972e-01 -9.95097645e-01 -9.94455956e-01 -9.93774928e-01\n",
      " -9.93054589e-01 -9.92294968e-01 -9.91496094e-01 -9.90657999e-01\n",
      " -9.89780716e-01 -9.88864280e-01 -9.87908727e-01 -9.86914095e-01\n",
      " -9.85880423e-01 -9.84807753e-01 -9.83696126e-01 -9.82545587e-01\n",
      " -9.81356181e-01 -9.80127955e-01 -9.78860957e-01 -9.77555239e-01\n",
      " -9.76210851e-01 -9.74827847e-01 -9.73406281e-01 -9.71946209e-01\n",
      " -9.70447691e-01 -9.68910783e-01 -9.67335548e-01 -9.65722048e-01\n",
      " -9.64070347e-01 -9.62380509e-01 -9.60652602e-01 -9.58886695e-01\n",
      " -9.57082856e-01 -9.55241158e-01 -9.53361672e-01 -9.51444475e-01\n",
      " -9.49489640e-01 -9.47497247e-01 -9.45467373e-01 -9.43400098e-01\n",
      " -9.41295506e-01 -9.39153678e-01 -9.36974699e-01 -9.34758657e-01\n",
      " -9.32505637e-01 -9.30215731e-01 -9.27889027e-01 -9.25525619e-01\n",
      " -9.23125599e-01 -9.20689063e-01 -9.18216107e-01 -9.15706829e-01\n",
      " -9.13161327e-01 -9.10579704e-01 -9.07962060e-01 -9.05308500e-01\n",
      " -9.02619128e-01 -8.99894051e-01 -8.97133376e-01 -8.94337213e-01\n",
      " -8.91505673e-01 -8.88638867e-01 -8.85736908e-01 -8.82799913e-01\n",
      " -8.79827996e-01 -8.76821275e-01 -8.73779870e-01 -8.70703900e-01\n",
      " -8.67593488e-01 -8.64448755e-01 -8.61269828e-01 -8.58056831e-01\n",
      " -8.54809891e-01 -8.51529138e-01 -8.48214700e-01 -8.44866709e-01\n",
      " -8.41485297e-01 -8.38070599e-01 -8.34622748e-01 -8.31141882e-01\n",
      " -8.27628139e-01 -8.24081656e-01 -8.20502575e-01 -8.16891037e-01\n",
      " -8.13247185e-01 -8.09571163e-01 -8.05863117e-01 -8.02123193e-01\n",
      " -7.98351539e-01 -7.94548304e-01 -7.90713639e-01 -7.86847695e-01\n",
      " -7.82950626e-01 -7.79022586e-01 -7.75063729e-01 -7.71074213e-01\n",
      " -7.67054195e-01 -7.63003834e-01 -7.58923291e-01 -7.54812728e-01\n",
      " -7.50672305e-01 -7.46502188e-01 -7.42302542e-01 -7.38073532e-01\n",
      " -7.33815325e-01 -7.29528091e-01 -7.25211999e-01 -7.20867219e-01\n",
      " -7.16493923e-01 -7.12092285e-01 -7.07662479e-01 -7.03204679e-01\n",
      " -6.98719062e-01 -6.94205806e-01 -6.89665089e-01 -6.85097090e-01\n",
      " -6.80501991e-01 -6.75879973e-01 -6.71231219e-01 -6.66555913e-01\n",
      " -6.61854240e-01 -6.57126385e-01 -6.52372537e-01 -6.47592882e-01\n",
      " -6.42787610e-01 -6.37956911e-01 -6.33100976e-01 -6.28219997e-01\n",
      " -6.23314168e-01 -6.18383682e-01 -6.13428734e-01 -6.08449521e-01\n",
      " -6.03446239e-01 -5.98419086e-01 -5.93368262e-01 -5.88293965e-01\n",
      " -5.83196397e-01 -5.78075760e-01 -5.72932255e-01 -5.67766086e-01\n",
      " -5.62577458e-01 -5.57366576e-01 -5.52133646e-01 -5.46878875e-01\n",
      " -5.41602472e-01 -5.36304643e-01 -5.30985600e-01 -5.25645553e-01\n",
      " -5.20284712e-01 -5.14903290e-01 -5.09501500e-01 -5.04079556e-01\n",
      " -4.98637671e-01 -4.93176062e-01 -4.87694944e-01 -4.82194534e-01\n",
      " -4.76675049e-01 -4.71136709e-01 -4.65579732e-01 -4.60004337e-01\n",
      " -4.54410746e-01 -4.48799180e-01 -4.43169861e-01 -4.37523010e-01\n",
      " -4.31858853e-01 -4.26177612e-01 -4.20479513e-01 -4.14764781e-01\n",
      " -4.09033642e-01 -4.03286322e-01 -3.97523050e-01 -3.91744053e-01\n",
      " -3.85949559e-01 -3.80139798e-01 -3.74315000e-01 -3.68475395e-01\n",
      " -3.62621214e-01 -3.56752688e-01 -3.50870051e-01 -3.44973534e-01\n",
      " -3.39063370e-01 -3.33139795e-01 -3.27203041e-01 -3.21253344e-01\n",
      " -3.15290939e-01 -3.09316061e-01 -3.03328948e-01 -2.97329837e-01\n",
      " -2.91318963e-01 -2.85296566e-01 -2.79262883e-01 -2.73218154e-01\n",
      " -2.67162616e-01 -2.61096510e-01 -2.55020076e-01 -2.48933554e-01\n",
      " -2.42837185e-01 -2.36731210e-01 -2.30615871e-01 -2.24491409e-01\n",
      " -2.18358066e-01 -2.12216086e-01 -2.06065711e-01 -1.99907185e-01\n",
      " -1.93740751e-01 -1.87566653e-01 -1.81385136e-01 -1.75196443e-01\n",
      " -1.69000820e-01 -1.62798512e-01 -1.56589764e-01 -1.50374822e-01\n",
      " -1.44153931e-01 -1.37927338e-01 -1.31695289e-01 -1.25458030e-01\n",
      " -1.19215809e-01 -1.12968871e-01 -1.06717465e-01 -1.00461838e-01\n",
      " -9.42022363e-02 -8.79389084e-02 -8.16721019e-02 -7.54020646e-02\n",
      " -6.91290446e-02 -6.28532900e-02 -5.65750492e-02 -5.02945704e-02\n",
      " -4.40121020e-02 -3.77278927e-02 -3.14421909e-02 -2.51552454e-02\n",
      " -1.88673048e-02 -1.25786178e-02 -6.28943332e-03 -1.22464680e-16]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1000 is out of bounds for axis 0 with size 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bs \u001b[38;5;241m=\u001b[39m BurgerSolver(tend\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, nu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, cfl\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 120\u001b[0m, in \u001b[0;36mBurgerSolver.solve\u001b[0;34m(self, pbar, verbose)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m### COMPLETE CODE HERE\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m,u)\n\u001b[0;32m--> 120\u001b[0m unew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance_u\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# FTUS solver.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# self.u holds the solutions for all times.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# u is the current solution.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# unew is the future solution, which you need to compute with finite differencing.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m unew\n",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m, in \u001b[0;36mBurgerSolver.advance_u\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     44\u001b[0m         alpha_arr[i] \u001b[38;5;241m=\u001b[39m u_i \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m dtdx \u001b[38;5;241m*\u001b[39m (u_i \u001b[38;5;241m-\u001b[39m u[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m u_i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m         alpha_arr[i] \u001b[38;5;241m=\u001b[39m u_i \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m dtdx \u001b[38;5;241m*\u001b[39m (\u001b[43mu\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m u_i))\n\u001b[1;32m     48\u001b[0m gamma_arr \u001b[38;5;241m=\u001b[39m alpha_arr \u001b[38;5;241m+\u001b[39m beta_arr\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gamma_arr\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1000 is out of bounds for axis 0 with size 1000"
     ]
    }
   ],
   "source": [
    "bs = BurgerSolver(tend=1, n=1000, nu=0.01, cfl=0.05)\n",
    "bs.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.plot_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.plot_evo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above space-time diagram is our dataset from which we will sample data points at random values of (x,t) that will be the inputs into our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network class for a single hidden layer\n",
    "You'll need to modify several parts of the code below, marked by `COMPLETE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    A neural network class with a single hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        solver,\n",
    "        num_training_unique=1000,\n",
    "        n_epochs=10,\n",
    "        learning_rate=0.1,\n",
    "        regularization_rate=0.05,\n",
    "        hidden_layer_size=2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialization routine for parameters, training data, and NN weights (matrices)\n",
    "        Inputs:\n",
    "        * solver :: PDE solver with a `random_sample` routine that returns a ModelData object.\n",
    "                    In HW7, it'd be a BurgerSolver object after you solve the system.\n",
    "        * num_training_unique :: Training dataset size\n",
    "        * n_epochs :: Number of training epochs, each randomly selecting `num_training_unique` inputs\n",
    "        * learning_rate :: Factor that softens the gradient descent during minimization\n",
    "        * regularization_rate :: Factor that controls how much the PDE residual is weighted (if used) in the loss function\n",
    "        * hidden_layer_size :: Number of nodes in the single hidden layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.solver = solver\n",
    "        self.num_training_unique = num_training_unique\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.train_set = self.solver.random_sample(self.num_training_unique)\n",
    "        self.normalize(set_limits=True)\n",
    "        self.set_boundary_points()\n",
    "        # learning rate\n",
    "        self.eta = learning_rate\n",
    "        self.lam = regularization_rate\n",
    "\n",
    "        # we get the size of the layers from the length of the input\n",
    "        # and output\n",
    "        model = self.train_set\n",
    "\n",
    "        # the number of nodes/neurons on the output layer\n",
    "        self.m = model.y.shape[0]\n",
    "\n",
    "        # the number of nodes/neurons on the input layer\n",
    "        self.n = model.x.shape[0]\n",
    "\n",
    "        # the number of nodes/neurons on the hidden layer\n",
    "        self.k = hidden_layer_size\n",
    "\n",
    "        # we will initialize the weights with Gaussian normal random\n",
    "        # numbers centered on 0 with a width of 1/sqrt(n), where n is\n",
    "        # the length of the input state\n",
    "\n",
    "        # A is the set of weights between the hidden layer and output layer\n",
    "        self.A = np.random.normal(0.0, 1.0 / np.sqrt(self.k), (self.m, self.k))\n",
    "\n",
    "        # B is the set of weights between the input layer and hidden layer\n",
    "        self.B = np.random.normal(0.0, 1.0 / np.sqrt(self.n), (self.k, self.n))\n",
    "\n",
    "    def normalize(self, model=None, set_limits=False):\n",
    "        \"\"\"\n",
    "        Training NNs typically work best when the inputs and outputs are normalized.\n",
    "        \"\"\"\n",
    "        if set_limits:\n",
    "            self.xmin = self.train_set.x.min(1)\n",
    "            self.xmax = self.train_set.x.max(1)\n",
    "            self.ymin = self.train_set.y.min()\n",
    "            self.ymax = self.train_set.y.max()\n",
    "        if model == None:\n",
    "            self.train_set.x = (self.train_set.x - self.xmin[:, None]) / (\n",
    "                self.xmax[:, None] - self.xmin[:, None]\n",
    "            )\n",
    "            self.train_set.y = (self.train_set.y - self.ymin) / (self.ymax - self.ymin)\n",
    "        else:\n",
    "            _model = copy.deepcopy(model)\n",
    "            _model.x = (_model.x - self.xmin[:, None]) / (\n",
    "                self.xmax[:, None] - self.xmin[:, None]\n",
    "            )\n",
    "            _model.y = (_model.y - self.ymin) / (self.ymax - self.ymin)\n",
    "            return _model\n",
    "\n",
    "    def denormalize(self, data):\n",
    "        \"\"\"\n",
    "        This function is used when we need to remove the normalization factors when providing predictions.\n",
    "        \"\"\"\n",
    "        data = data * (self.ymax - self.ymin) + self.ymin\n",
    "        return data\n",
    "\n",
    "    def set_activation(self, gtype=\"sigmoid\"):\n",
    "        if gtype not in [\"sigmoid\", \"relu\", \"tanh\", \"leaky_relu\"]:\n",
    "            raise RuntimeError(f\"Activation function {gtype} unknown.\")\n",
    "        self.gtype = gtype\n",
    "\n",
    "    def g(self, p, type=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        our activation function that operates on the hidden layer.\n",
    "        NOTE: AFAIK in this code, sigmoid is the only function that works in Burger's Equation for 1 hidden layer.\n",
    "        \"\"\"\n",
    "        if self.gtype == \"sigmoid\":\n",
    "            return 1.0 / (1.0 + np.exp(-p))\n",
    "        elif self.gtype == \"relu\":\n",
    "            return np.maximum(0, p)\n",
    "        elif self.gtype == \"leaky_relu\":\n",
    "            return np.maximum(0.1 * p, p)\n",
    "        elif self.gtype == \"tanh\":\n",
    "            return np.tanh(p)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Activation function {self.gtype} unknown.\")\n",
    "\n",
    "    def set_boundary_points(self, N=50):\n",
    "        \"\"\"\n",
    "        Sets points for the initial and boundary conditions so we can use them in the loss function.\n",
    "        \"\"\"\n",
    "        self.bc_x = np.linspace(self.xmin[0], self.xmax[0], N)\n",
    "        self.bc_t = np.linspace(self.xmin[1], self.xmax[1], N)\n",
    "\n",
    "        ###\n",
    "        ### COMPLETE HERE.  Set the analytic functions that describe the initial and boundary conditions.\n",
    "        ###\n",
    "        self.initial_fn = lambda x: None\n",
    "        self.boundary_fn = lambda x: None\n",
    "\n",
    "    def return_ic_loss(self):\n",
    "        \"\"\"\n",
    "        Returns the mean squared error between the predictions and analytic initial condition.\n",
    "        \"\"\"\n",
    "        ###\n",
    "        ### COMPLETE HERE.\n",
    "        ###\n",
    "        return 0.0\n",
    "\n",
    "    def return_bc_loss(self):\n",
    "        \"\"\"\n",
    "        Returns the mean squared error between the predictions and analytic boundary condition.\n",
    "        \"\"\"\n",
    "        ###\n",
    "        ### COMPLETE HERE.\n",
    "        ###\n",
    "        return 0.0\n",
    "\n",
    "    def return_deriv(self, x0, t0, dx=1e-3, dt=1e-3):\n",
    "        \"\"\"\n",
    "        Given a (x,t) value, this function retuns the following partial derivatives so we can compute the residual from Burger's Equation.\n",
    "        Inputs:\n",
    "        * x0 :: position to evaluate derivatives\n",
    "        * t0 :: time to evaluate derivatives\n",
    "        * dx :: distance between adjacent points when computing spatial derivatives\n",
    "        * dt :: time between adjacent points when computing time derivatives\n",
    "\n",
    "        Returns:\n",
    "        * u_t :: du/dt\n",
    "        * u_x :: du/dx\n",
    "        * u_xx :: d^2 u/dx^2\n",
    "        \"\"\"\n",
    "        ###\n",
    "        ### COMPLETE HERE\n",
    "        ###\n",
    "        # Use the NN to predict the u-values at nearby points and use central differencing to compute all the needed derivatives\n",
    "\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    def return_res_loss(self, u, x, t):\n",
    "        \"\"\"\n",
    "        Returns residual from Burger's equation: u_t + u*u_x - nu*u_xx = 0\n",
    "        \"\"\"\n",
    "        u_t, u_x, u_xx = self.return_deriv(x, t)\n",
    "        res = u_t + u * u_x - self.eta * u_xx\n",
    "        return np.atleast_2d(res)\n",
    "\n",
    "    def set_loss_function(self, fn=None, method=\"exact\"):\n",
    "        if fn is not None:\n",
    "            self.loss = fn\n",
    "        else:\n",
    "            if method == \"exact\":\n",
    "                self.loss = lambda x, y, z: z - y\n",
    "            # Add your methods here or assign a function\n",
    "            elif method == \"exact+initial\":\n",
    "                self.loss = lambda x, y, z: z - y + self.return_ic_loss()\n",
    "            elif method == \"exact+initial+boundary\":\n",
    "                self.loss = (\n",
    "                    lambda x, y, z: z - y\n",
    "                    + self.return_ic_loss()\n",
    "                    + self.return_bc_loss()\n",
    "                )\n",
    "            elif method == \"res+initial+boundary\":\n",
    "                self.loss = (\n",
    "                    lambda x, y, z: z - y\n",
    "                    + self.lam * self.return_res_loss(y[0], x[0], x[1])\n",
    "                    + self.return_ic_loss()\n",
    "                    + self.return_bc_loss()\n",
    "                )\n",
    "            else:\n",
    "                raise RuntimeError(f\"Method {method} not recognized.\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the neural network by doing gradient descent with back\n",
    "        propagation to set the matrix elements in B (the weights\n",
    "        between the input and hidden layer) and A (the weights between\n",
    "        the hidden layer and output layer)\n",
    "        \"\"\"\n",
    "        all_loss = []\n",
    "        print(\n",
    "            \"0|\" + \"-\" * 25 + \"|\" + \"-\" * 25 + \"|\" + \"-\" * 25 + \"|\" + \"-\" * 25 + \"|100\"\n",
    "        )\n",
    "        for iepoch in range(self.n_epochs):\n",
    "            # print(f\"epoch {i+1} of {self.n_epochs}\")\n",
    "            loss = 0.0\n",
    "            for _ in range(self.num_training_unique):\n",
    "                ii = np.random.randint(0, self.num_training_unique)\n",
    "\n",
    "                # Convert into 1D\n",
    "                x = self.train_set.x[:, ii].reshape(self.n, 1)\n",
    "                y = self.train_set.y[:, ii].reshape(self.m, 1)\n",
    "\n",
    "                z_tilde = self.g(self.B @ x)\n",
    "                z = self.g(self.A @ z_tilde)\n",
    "\n",
    "                e = self.loss(x, y, z)\n",
    "                loss += (e**2).sum()\n",
    "                if np.isinf(loss):\n",
    "                    raise RuntimeError(\n",
    "                        f\"Infinite loss function. Epoch {iepoch}, e = {e}, x,y = {x},{y}\"\n",
    "                    )\n",
    "                e_tilde = self.A.T @ e\n",
    "\n",
    "                dA = -2 * self.eta * e * z * (1 - z) @ z_tilde.T\n",
    "                dB = -2 * self.eta * e_tilde * z_tilde * (1 - z_tilde) @ x.T\n",
    "\n",
    "                self.A[:, :] += dA\n",
    "                self.B[:, :] += dB\n",
    "\n",
    "            print(\"  \" + \"=\" * int(iepoch / self.n_epochs * 100) + \">\", end=\"\\r\")\n",
    "            all_loss.append(loss)\n",
    "        return np.array(all_loss) / self.num_training_unique\n",
    "\n",
    "    def predict(self, model):\n",
    "        \"\"\"predict the outcome using our trained matrix A\"\"\"\n",
    "        nmodel = self.normalize(model=model)\n",
    "        y = self.g(self.A @ (self.g(self.B @ nmodel.x)))\n",
    "        return self.denormalize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(\n",
    "    bs, num_training_unique=2000, n_epochs=50, hidden_layer_size=40, learning_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.set_activation(\"sigmoid\")\n",
    "nn.set_loss_function(method=\"exact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
