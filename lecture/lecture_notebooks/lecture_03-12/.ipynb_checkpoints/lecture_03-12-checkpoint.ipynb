{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75882242-9d56-4853-a6a7-72cb50a9e8eb",
   "metadata": {},
   "source": [
    "# Lecture 03/10: Introduction to Machine Learning\n",
    "## Neural Networks\n",
    "### Neural Network Overview\n",
    "\n",
    "#### Structure of a Neural Net\n",
    "A neural net includes several steps, divided into _layers_:\n",
    "1. Input Layer (no processing, takes in input)\n",
    "2. Processing with map into output Layer\n",
    "\n",
    "Data is usually normalized onto the unit interval.\n",
    "\n",
    "Processing involves applying some (nonlinear) function onto the mapped data:\n",
    "\n",
    "Linear Mapping: for input $\\vec{x}$, can we predict the output $\\vec{z}$:\n",
    "\n",
    "$g(\\vec{x})$ is some function applied to the mapping $\\mathbb{A}$,\n",
    "\n",
    "$\\vec{z} = \\mathbb{A} \\cdot \\vec{x}$, such that\n",
    "\n",
    "$\\vec{z} = g( \\mathbb{A} \\cdot \\vec{x} )$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e64cea-f272-44b3-b983-440c26a74750",
   "metadata": {},
   "source": [
    "#### Choice of mapping $g$\n",
    "- Mimicing the behavior of \"neurons\" (taken to be the connections between input and output data), some nonlinear map chooses whether a neuron \"fires\" and how sensitive that is to the input\n",
    "- Typical choice: Sigmoid function\n",
    "$g(p) = \\frac{1}{1+e^{-\\alpha p}}$, such that $z\\in[0,1]$\n",
    "- Common choice is $\\alpha=1$\n",
    "- There is a narrow range of non-linearity, so we can also pick $\\alpha$ such that the inputs fall in the nonlinear range\n",
    "$\\alpha = \\frac{10}{n \\max{|x_i|}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f1916-2f7e-4f41-874c-e1efa490e1fc",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "#### Training\n",
    "- We have $T$ pairs of $(x_k,y_k)$ coordinates that we can use to train our model\n",
    "- Important: remember that our y-values have to be scaled to (0,1), so they are in the same range that our function $g(p)$ maps to\n",
    "- Recall that $g$ is a scalar function:\n",
    "  $g(\\mathbb{A}\\cdot \\vec{x}) = \\vec{y}$\n",
    "  $\\implies z_i = g([\\mathbb{A}\\cdot\\vec{x}]_i) = g\\left( \\sum_j A_{ij} x_j \\right)$\n",
    "\n",
    "- We seek the elements of $\\mathbb{A}$\n",
    "- This can be expressed as a minimization problem, where we alter the matrix elements to achieve this agreement:\n",
    "- Defining the loss function\n",
    "$f(A_{ij}) = \\left| g(\\mathbb{A} x^k) - y^k \\right|^2$\n",
    "- The learning is then perscribed by:\n",
    "$\\vec{x} \\leftarrow x - \\eta \\nabla u$\n",
    "\n",
    "We have the following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e3137-5ddf-4746-92d0-547811a456f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of steepest (gradient) descent minimization\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rosenbrock(x0, x1, a, b):\n",
    "    return (a - x0)**2 + b*(x1 - x0**2)**2\n",
    "\n",
    "def drosdx(x, a, b):\n",
    "    x0 = x[0]\n",
    "    x1 = x[1]\n",
    "    return np.array([-2.0*(a - x0) - 4.0*b*x0*(x1 - x0**2),\n",
    "                     2.0*b*(x1 - x0**2)])\n",
    "\n",
    "def inside(x, bounds):\n",
    "    return ((x > bounds[:,0]) & (x < bounds[:,1])).all()\n",
    "\n",
    "def main():\n",
    "\n",
    "    xmin = -2.0\n",
    "    xmax = 2.0\n",
    "    ymin = -1.0\n",
    "    ymax = 3.0\n",
    "\n",
    "    a = 1.0\n",
    "    b = 100.0\n",
    "\n",
    "    N = 256\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "    y = np.linspace(ymin, ymax, N)\n",
    "\n",
    "    x2d, y2d = np.meshgrid(x, y, indexing=\"ij\")\n",
    "\n",
    "    plt.imshow(np.log10(np.transpose(rosenbrock(x2d, y2d, a, b))), \n",
    "               origin=\"lower\",\n",
    "               extent=[xmin, xmax, ymin, ymax])\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"min_2d_start.png\", dpi=150)\n",
    "\n",
    "\n",
    "    # do descent\n",
    "    xp = np.array([-1.0, 1.5])\n",
    "    xp_old = 1000*xp\n",
    "\n",
    "    eps = 1.e-5\n",
    "\n",
    "    eta = 0.002\n",
    "\n",
    "\n",
    "    \n",
    "    iter = 0\n",
    "    bounds = np.array([[xmin, xmax], [ymin, ymax]])\n",
    "    while np.linalg.norm(xp - xp_old) > eps and inside(xp, bounds):\n",
    "        xp_old[:] = xp[:]\n",
    "\n",
    "        ### In-class problem: Complete your implementation of steepest descent here\n",
    "        grad_ = drosdx(xp_old, a, b)\n",
    "        xp = xp_old - eta * grad_\n",
    "\n",
    "        iter += 1\n",
    "        if iter % 500 == 0:\n",
    "            print(f\"Iteration {iter}: Completed\")\n",
    "    \n",
    "        plt.plot([xp_old[0], xp[0]], [xp_old[1], xp[1]], color=\"C1\")\n",
    "\n",
    "    plt.scatter([xp[0]], [xp[1]], marker=\"o\", color=\"C1\")    \n",
    "    if not inside(xp, bounds):\n",
    "        print(f\"Exited because point {xp} exited the allocated bounds {bounds}\")\n",
    "        plt.title(\"Failed\")\n",
    "    else:\n",
    "        print(f\"Found minimum {xp} in {iter} iterations\")\n",
    "        plt.title(\"Success\")\n",
    "\n",
    "    plt.savefig(f\"min_2d_descent_eta-{eta}.png\", dpi=150)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aba679-ba37-4d72-9eba-9dde03d05e7a",
   "metadata": {},
   "source": [
    "#### Caveats\n",
    "- When you minimize with one set of training data, there is no guarantee that you are still minimized with respect to the previous sets\n",
    "- In practice, you feed the training data multiple times, in random order to the minimizer\n",
    "- Each pass is called an epoch\n",
    "\n",
    "#### Neural Net Minimization\n",
    "We are minimizing the function\n",
    "$$ f(A_{ij}) = |g(Ax^k) - y^k|^2 $$\n",
    "such that\n",
    "$$ (x^k,y^k) = ([x_1^k, x_2^k, \\dots, x_n^k], [y_1^k, y_2^k, \\dots, y_n^k]) \\qquad .$$\n",
    "Each update would be:\n",
    "$$ A^{new}_{pq} = A_{pq} - \\eta \\frac{\\partial f}{\\partial A_{pq} } $$\n",
    "so\n",
    "$$ f(A_{ij}) = \\sum_{i=1}^m \\left [ g\\left(\\sum_{j=1}^n A_{ij} x_j \\right ) - y_i\\right ]^2 $$\n",
    "\n",
    "The derivative $\\frac{\\partial f}{\\partial A_{ij}}$ is given by \n",
    "$$\\frac{\\partial f}{\\partial A_{ij}} = 2(z_p - y_p) \\alpha z_p (1-z_p)x_q$$\n",
    "where $\\vec{z} = \\mathbb{A}\\cdot\\vec{x}$ and $\\alpha$ is the sigmoid parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08523c-9b1c-41f1-9977-0f400024ef76",
   "metadata": {},
   "source": [
    "## Hidden Layers\n",
    "\n",
    "The first and last layer are the input and output layers, respectively.\\\n",
    "Any intermediate steps are denoted _Hidden Layers_ in the neural net. Any neural net which has a hidden layer is a _deep_ network.\\\n",
    "Consider a network which has a single hidden layer, the transformation described by the matrix $\\mathbb{B}$.\\\n",
    "Then, minimization of the (here, $L^2$) error is done in a backwards order, in practice: this is known as _back-propagation_. The error is given by \n",
    "$$ E = \\sum (z - y)^2 $$\n",
    "where $z$ is the prediction and $y$ is the true value.\\\n",
    "The backpropagation technique using gradient descent performs the gradient descent algorithm on $\\mathbb{A}$ and $\\mathbb{B}$ _separately_. Output error backpropagates to the intermediate, hidden layer, and the hidden layer error backpropagates to the input.\n",
    "\n",
    "#### In practice\n",
    "- usually only a single hidden layer is needed\n",
    "- the number of nodes in each layer is also not normally fixed\n",
    "- it is common practice to \"down-select\" to only the most important variables in the hidden layer\n",
    "- then, usually only a single variable or few sets of variables is necessary to output\n",
    "- See tensorflow demo online\n",
    "- Using the loss function as a function of epoch is revealing to the nature of the fit\n",
    "\n",
    "  #### Neural Net Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf8228f-6a78-4d90-862f-a402363376f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained matrix: \n",
      "[[-0.63309899 -0.39997684 -0.52972595 -0.34768682 -0.25831159 -0.39445251\n",
      "  -0.6282089  -0.49063745 -0.74369607  4.5650922 ]]\n",
      " \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 171\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m n_right \u001b[38;5;241m/\u001b[39m npts\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m#f = main(use_alpha=False)\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m#print(\"frac: {}\".format(f))\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_scipy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrac: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(f))\n",
      "Cell \u001b[0;32mIn[1], line 144\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(use_scipy, use_alpha, eta)\u001b[0m\n\u001b[1;32m    141\u001b[0m     err\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(y_nn \u001b[38;5;241m-\u001b[39m model\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    143\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[0;32m--> 144\u001b[0m markerline, stemlines, baseline \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinefmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m plt\u001b[38;5;241m.\u001b[39msetp(stemlines, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_data\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m plt_suffix, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/pyplot.py:2873\u001b[0m, in \u001b[0;36mstem\u001b[0;34m(linefmt, markerfmt, basefmt, bottom, label, use_line_collection, orientation, data, *args)\u001b[0m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mstem)\n\u001b[1;32m   2869\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstem\u001b[39m(\n\u001b[1;32m   2870\u001b[0m         \u001b[38;5;241m*\u001b[39margs, linefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, markerfmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, basefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, bottom\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2871\u001b[0m         label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_line_collection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, orientation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2872\u001b[0m         data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 2873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinefmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinefmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarkerfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarkerfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasefmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasefmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbottom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2876\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_line_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_line_collection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2877\u001b[0m \u001b[43m        \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2878\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1414\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1415\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1416\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axes/_axes.py:2919\u001b[0m, in \u001b[0;36mAxes.stem\u001b[0;34m(self, linefmt, markerfmt, basefmt, bottom, label, use_line_collection, orientation, *args)\u001b[0m\n\u001b[1;32m   2916\u001b[0m     baseline_x \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mmin(locs), np\u001b[38;5;241m.\u001b[39mmax(locs)]\n\u001b[1;32m   2917\u001b[0m     baseline_y \u001b[38;5;241m=\u001b[39m [bottom, bottom]\n\u001b[0;32m-> 2919\u001b[0m markerline, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarker_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2920\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarkercolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinestyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarkerstyle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2921\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarkermarker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_nolegend_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2923\u001b[0m baseline, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot(baseline_x, baseline_y,\n\u001b[1;32m   2924\u001b[0m                       color\u001b[38;5;241m=\u001b[39mbasecolor, linestyle\u001b[38;5;241m=\u001b[39mbasestyle,\n\u001b[1;32m   2925\u001b[0m                       marker\u001b[38;5;241m=\u001b[39mbasemarker, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_nolegend_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2927\u001b[0m stem_container \u001b[38;5;241m=\u001b[39m StemContainer((markerline, stemlines, baseline),\n\u001b[1;32m   2928\u001b[0m                                label\u001b[38;5;241m=\u001b[39mlabel)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1632\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axes/_base.py:312\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    311\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axes/_base.py:498\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a neural network example.  We take an input vector of 10 numbers and\n",
    "# want the net to return simply the last number.\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# we will restrict the numbers in our sample to be drawn from this set\n",
    "NUM_SET = [0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95]\n",
    "\n",
    "class ModelData(object):\n",
    "    \"\"\"this is the model data for our \"last number\" training set.  We\n",
    "    produce input of length N, drawing from the NUM_SET randomly and\n",
    "    then we set the output to be simply the last element of the input\n",
    "    vector\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, N=10):\n",
    "        self.N = N\n",
    "\n",
    "        # our model input data\n",
    "        self.x = np.array([random.choice(NUM_SET) for q in range(self.N)])\n",
    "\n",
    "        # our scaled model output data\n",
    "        self.y = np.array([self.x[-1]])\n",
    "\n",
    "    def round_to_allowed(self, out):\n",
    "        \"\"\"take the network output and return the number from the allowed\n",
    "        sequence we are closest to\n",
    "\n",
    "        \"\"\"\n",
    "        return min(NUM_SET, key=lambda q:abs(q - out))\n",
    "\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, num_training_unique=100, n_epochs=10,\n",
    "                 use_alpha=True, eps=1.e-8, eta=0.1):\n",
    "        self.num_training_unique = num_training_unique\n",
    "        self.n_epochs = n_epochs\n",
    "        self.eps = eps\n",
    "        self.eta = eta\n",
    "\n",
    "        self.train_set = []\n",
    "        for _ in range(self.num_training_unique):\n",
    "            self.train_set.append(ModelData())\n",
    "\n",
    "        # initialize our matrix with Gaussian normal random numbers\n",
    "        # we get the size from the length of the input and output\n",
    "        # Gaussian dist chosen for init because it may lead to faster training\n",
    "        model = self.train_set[0]\n",
    "        self.m = len(model.y)\n",
    "        self.n = len(model.x)\n",
    "\n",
    "        self.A = np.random.normal(0.0, 1.0/np.sqrt(self.n), (self.m, self.n))\n",
    "\n",
    "        if use_alpha:\n",
    "            self.compute_alpha()\n",
    "        else:\n",
    "            self.alpha = 1.0\n",
    "\n",
    "    def compute_alpha(self):\n",
    "        \"\"\"figure out a good value for alpha\"\"\"\n",
    "        max_x = -1.0\n",
    "        for model in self.train_set:\n",
    "            max_x = max(max_x, np.abs(model.x).max())\n",
    "\n",
    "        self.alpha = 10.0/(self.n * max_x)\n",
    "\n",
    "    def g(self, p):\n",
    "        \"\"\"our sigmoid function\"\"\"\n",
    "        return 1.0/(1.0 + np.exp(-self.alpha*p))\n",
    "\n",
    "    def f_minimize(self, A_elements, x, y):\n",
    "        \"\"\" This is the function we want to minimize for training\"\"\"\n",
    "        A = A_elements.reshape((self.m, self.n))\n",
    "        z = self.g(A @ x)\n",
    "\n",
    "        return (z - y) @ (z - y)\n",
    "\n",
    "    def train(self, use_scipy=True):\n",
    "        \"\"\"Do the minimization for the training using SciPy's minimization\"\"\"\n",
    "\n",
    "        # train\n",
    "        for _ in range(self.n_epochs*len(self.train_set)):\n",
    "            model = random.choice(self.train_set)\n",
    "\n",
    "            if use_scipy:\n",
    "                res = optimize.minimize(self.f_minimize, self.A.flatten(),\n",
    "                                        args=(model.x, model.y), tol=self.eps)\n",
    "\n",
    "                if not res.success:\n",
    "                    sys.exit(\"training optimization failed\")\n",
    "\n",
    "                self.A[:,:] = res.x.reshape((len(model.y), len(model.x)))\n",
    "\n",
    "            else:\n",
    "                # gradient descent -- just a single improvement.  eta\n",
    "                # here is our learning rate\n",
    "\n",
    "                # make these column vectors\n",
    "                x = model.x.reshape(self.n, 1)\n",
    "                y = model.y.reshape(self.m, 1)\n",
    "\n",
    "                b = self.A @ x\n",
    "                z = self.g(b)\n",
    "\n",
    "                self.A[:,:] += -self.eta * 2*self.alpha*(z - y)*z*(1 - z) @ x.T\n",
    "\n",
    "\n",
    "    def predict(self, model):\n",
    "        \"\"\" predict the outcome using our trained matrix A \"\"\"\n",
    "        z = self.g(self.A @ model.x)\n",
    "        return model.round_to_allowed(z)\n",
    "\n",
    "\n",
    "def main(use_scipy=True, use_alpha=True, eta=0.1):\n",
    "\n",
    "    if use_scipy:\n",
    "        plt_suffix = \"_scipy.png\"\n",
    "    else:\n",
    "        plt_suffix = \"_grad_descent.png\"\n",
    "\n",
    "    # length of our input vector\n",
    "    # initialize neural network\n",
    "    nn = NeuralNetwork(num_training_unique=100, n_epochs=100, use_alpha=use_alpha, eta=eta)\n",
    "\n",
    "    # train using class \n",
    "    nn.train(use_scipy=use_scipy)\n",
    "\n",
    "    print(\"trained matrix: \")\n",
    "    print(nn.A)\n",
    "    print(\" \")\n",
    "\n",
    "    # try it out -- first on our original training set data\n",
    "    err = []\n",
    "    for model in nn.train_set:\n",
    "        y_nn = nn.predict(model)\n",
    "        err.append(float(y_nn - model.x[-1]))\n",
    "\n",
    "    plt.clf()\n",
    "    markerline, stemlines, baseline = plt.stem(err, \":\", linefmt=\"C0\")\n",
    "    plt.setp(stemlines, \"color\", \"C0\")\n",
    "    plt.savefig(\"trained_data\" + plt_suffix, dpi=150)\n",
    "\n",
    "    # now try it out on 100 different new random sequences\n",
    "    err = []\n",
    "    npts = 100\n",
    "    n_right = 0\n",
    "    for k in range(npts):\n",
    "        model = ModelData()\n",
    "        y_nn = nn.predict(model)\n",
    "        e = float(y_nn - model.x[-1])\n",
    "        if e == 0:\n",
    "            n_right += 1\n",
    "        err.append(e)\n",
    "\n",
    "    plt.clf()\n",
    "    markerline, stemlines, baseline = plt.stem(err, \":\", linefmt=\"C0\")\n",
    "    plt.setp(stemlines, \"color\", \"C0\")\n",
    "    plt.savefig(\"random_data\" + plt_suffix, dpi=150)\n",
    "\n",
    "    return n_right / npts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #f = main(use_alpha=False)\n",
    "    #print(\"frac: {}\".format(f))\n",
    "\n",
    "    f = main(use_scipy=False, use_alpha=False, eta=0.1)\n",
    "    print(\"frac: {}\".format(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681a51e-21c9-433a-bda1-03d1d4f03dea",
   "metadata": {},
   "source": [
    "### Example: High Signal-to-Noise Ratio\n",
    "Consider a noisy signal we expect to lie in one of a few discrete frequency bands $f\\in\\{1,2,3,4\\}$.\\\n",
    "The signal is of the form\n",
    "$$ S = \\cos{(2\\pi f t)} + 5 r_i $$\n",
    "where $r_i$ is a random number on the interval $[-1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc654090-8525-4e98-a0a1-b656e42a543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a neural network w/ hidden layers example.  We feed in noisy data with one\n",
    "# of four frequencies and we want the net to return the frequency.\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ALLOWED_FREQS = [1, 2, 3, 4]\n",
    "\n",
    "class SignalData(object):\n",
    "    \"\"\"this is the model data for our frequency training set.  We produce\n",
    "    a signal (cosine with one of the allowed frequencies) and pollute\n",
    "    it with noise.  The output is an array with an 1 in the slot\n",
    "    corresponding to the correct frequency.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, N=101):\n",
    "        self.N = N\n",
    "\n",
    "        self.t = np.linspace(0.0, 1.0, N)\n",
    "\n",
    "        # frequency\n",
    "        self.f = random.choice(ALLOWED_FREQS)\n",
    "\n",
    "        # random numbers in (-1, 1)\n",
    "        noise = 2.0*np.random.random(N) - 1.0\n",
    "\n",
    "        # input data\n",
    "        self.x_orig = np.cos(2.0*np.pi*self.f*self.t)\n",
    "        self.x = self.x_orig + 5.0*noise\n",
    "\n",
    "        # output (scaled)\n",
    "        self.y = np.zeros((len(ALLOWED_FREQS))) + 0.01\n",
    "        self.y[self.f-1] = 1.0\n",
    "\n",
    "    def interpret_frequency(self, out):\n",
    "        \"\"\"given the output from the neural network, determine which frequency\n",
    "        was most preferred.\n",
    "\n",
    "        \"\"\"\n",
    "        return ALLOWED_FREQS[np.argmax(out)]\n",
    "\n",
    "    def plot(self, oname=\"model.png\"):\n",
    "        \"\"\"plot the signal data\"\"\"\n",
    "        plt.clf()\n",
    "        plt.scatter(self.t, self.x, color=\"C0\")\n",
    "        plt.plot(self.t, self.x_orig, color=\"C1\", ls=\":\", label=\"f = {}\".format(self.f))\n",
    "        plt.legend(frameon=False)\n",
    "        plt.savefig(oname)\n",
    "\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    \"\"\"A neural network class with a single hidden layer.\"\"\"\n",
    "\n",
    "    def __init__(self, num_training_unique=100, n_epochs=10,\n",
    "                 learning_rate=0.1,\n",
    "                 hidden_layer_size=100):\n",
    "\n",
    "        self.num_training_unique = num_training_unique\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.train_set = []\n",
    "        for _ in range(self.num_training_unique):\n",
    "            self.train_set.append(SignalData())\n",
    "\n",
    "        # learning rate\n",
    "        self.eta = learning_rate\n",
    "\n",
    "        # we get the size of the layers from the length of the input\n",
    "        # and output\n",
    "        model = self.train_set[0]\n",
    "\n",
    "        # the number of nodes/neurons on the output layer\n",
    "        self.m = len(model.y)\n",
    "\n",
    "        # the number of nodes/neurons on the input layer\n",
    "        self.n = len(model.x)\n",
    "\n",
    "        # the number of nodes/neurons on the hidden layer\n",
    "        self.k = hidden_layer_size\n",
    "\n",
    "        # we will initialize the weights with Gaussian normal random\n",
    "        # numbers centered on 0 with a width of 1/sqrt(n), where n is\n",
    "        # the length of the input state\n",
    "\n",
    "        # A is the set of weights between the hidden layer and output layer\n",
    "        self.A = np.random.normal(0.0, 1.0/np.sqrt(self.k), (self.m, self.k))\n",
    "\n",
    "        # B is the set of weights between the input layer and hidden layer\n",
    "        self.B = np.random.normal(0.0, 1.0/np.sqrt(self.n), (self.k, self.n))\n",
    "\n",
    "    def g(self, p):\n",
    "        \"\"\"our sigmoid function that operates on the hidden layer\"\"\"\n",
    "        return 1.0/(1.0 + np.exp(-p))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the neural network by doing gradient descent with back\n",
    "        propagation to set the matrix elements in B (the weights\n",
    "        between the input and hidden layer) and A (the weights between\n",
    "        the hidden layer and output layer)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(self.n_epochs):\n",
    "\n",
    "            print(\"epoch {} of {}\".format(i+1, self.n_epochs))\n",
    "\n",
    "            for _ in range(self.num_training_unique):\n",
    "\n",
    "                model = random.choice(self.train_set)\n",
    "\n",
    "                x = model.x.reshape(self.n, 1)\n",
    "                y = model.y.reshape(self.m, 1)\n",
    "\n",
    "                # take the intermediate layers, apply the sigmoid function\n",
    "                z_tilde = self.g(self.B @ x)\n",
    "                z = self.g(self.A @ z_tilde)\n",
    "\n",
    "                # calculate the errors for each layer\n",
    "                e = z - y\n",
    "                e_tilde = self.A.T @ e\n",
    "\n",
    "                # update matrix A, B with new values based on error\n",
    "                dA = -2*self.eta * e * z*(1-z) @ z_tilde.T\n",
    "                dB = -2*self.eta * e_tilde * z_tilde*(1-z_tilde) @ x.T\n",
    "\n",
    "                # Modify A,B\n",
    "                self.A[:, :] += dA\n",
    "                self.B[:, :] += dB\n",
    "\n",
    "    def predict(self, model):\n",
    "        \"\"\" predict the outcome using our trained matrix A \"\"\"\n",
    "        # sigmoid(output to hidden layer * sigmoid(hidden layer to input) )\n",
    "        y = self.g(self.A @ (self.g(self.B @ model.x)))\n",
    "        return y\n",
    "\n",
    "def main(k=2):\n",
    "\n",
    "    # length of our input vector\n",
    "    nn = NeuralNetwork(num_training_unique=1000, n_epochs=5, hidden_layer_size=k,\n",
    "                       learning_rate=0.05)\n",
    "\n",
    "    # train\n",
    "    nn.train()\n",
    "\n",
    "    # try it out -- first on our original training set data\n",
    "    err = []\n",
    "    for q, model in enumerate(nn.train_set):\n",
    "        y_nn = nn.predict(model)\n",
    "        err.append(abs(model.interpret_frequency(y_nn) - model.f))\n",
    "        if q == 0:\n",
    "            model.plot()\n",
    "\n",
    "    plt.clf()\n",
    "    markerline, stemlines, baseline = plt.stem(err, \":\", linefmt=\"C0\")\n",
    "    plt.setp(stemlines, \"color\", \"C0\")\n",
    "    plt.savefig(\"trained_data_{}.png\".format(k), dpi=150)\n",
    "\n",
    "\n",
    "    plt.clf()\n",
    "    bins = list(range(5))\n",
    "    plt.hist(err, bins=bins)\n",
    "    #bins_labels(bins)\n",
    "    plt.savefig(\"trained_hist_{}.png\".format(k), dpi=150)\n",
    "\n",
    "    # now try it out on 100 different new random sequences\n",
    "    err = []\n",
    "    for _ in range(100):\n",
    "        model = SignalData()\n",
    "        y_nn = nn.predict(model)\n",
    "        err.append(abs(model.interpret_frequency(y_nn) - model.f))\n",
    "\n",
    "    plt.clf()\n",
    "    markerline, stemlines, baseline = plt.stem(err, \":\", linefmt=\"C0\")\n",
    "    plt.setp(stemlines, \"color\", \"C0\")\n",
    "    plt.savefig(\"random_data_{}.png\".format(k), dpi=150)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.hist(err, bins=bins)\n",
    "    #bins_labels(bins)\n",
    "    plt.savefig(\"random_hist_{}.png\".format(k), dpi=150)\n",
    "\n",
    "\n",
    "def bins_labels(bins, **kwargs):\n",
    "    \"\"\"pretty histogram bin labels from\n",
    "    https://stackoverflow.com/questions/23246125/how-to-center-labels-in-histogram-plot\"\"\"\n",
    "    bin_w = (max(bins) - min(bins)) / (len(bins) - 1)\n",
    "    plt.xticks(np.arange(min(bins)+bin_w/2, max(bins), bin_w), bins, **kwargs)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for hidden_size in [1, 2, 4, 8, 16, 32, 64]:\n",
    "        main(k=hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a3b239-2e1a-4103-825e-6055f69fa9fd",
   "metadata": {},
   "source": [
    "#### Result: Maybe Neural Nets aren't always the best\n",
    "- a simple method of taking the FFT and returning signal with max power outperforms NN\n",
    "- However, there are caveats (see lecture slides)\n",
    "\n",
    "## Deep Learning (After Spring Break)\n",
    "- Any network with many (nowadays, 100s of) hidden layers\n",
    "- Free textbooks available on canvas\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463be5ed-6af1-4d3f-a8c7-24b6e5bd7ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
