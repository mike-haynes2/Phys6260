{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75882242-9d56-4853-a6a7-72cb50a9e8eb",
   "metadata": {},
   "source": [
    "# Lecture 03/10: Introduction to Machine Learning\n",
    "## Neural Networks\n",
    "### Neural Network Overview\n",
    "\n",
    "#### Structure of a Neural Net\n",
    "A neural net includes several steps, divided into _layers_:\n",
    "1. Input Layer (no processing, takes in input)\n",
    "2. Processing with map into output Layer\n",
    "\n",
    "Data is usually normalized onto the unit interval.\n",
    "\n",
    "Processing involves applying some (nonlinear) function onto the mapped data:\n",
    "\n",
    "Linear Mapping: for input $\\vec{x}$, can we predict the output $\\vec{z}$:\n",
    "\n",
    "$g(\\vec{x})$ is some function applied to the mapping $\\mathbb{A}$,\n",
    "\n",
    "$\\vec{z} = \\mathbb{A} \\cdot \\vec{x}$, such that\n",
    "\n",
    "$\\vec{z} = g( \\mathbb{A} \\cdot \\vec{x} )$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e64cea-f272-44b3-b983-440c26a74750",
   "metadata": {},
   "source": [
    "#### Choice of mapping $g$\n",
    "- Mimicing the behavior of \"neurons\" (taken to be the connections between input and output data), some nonlinear map chooses whether a neuron \"fires\" and how sensitive that is to the input\n",
    "- Typical choice: Sigmoid function\n",
    "$g(p) = \\frac{1}{1+e^{-\\alpha p}}$, such that $z\\in[0,1]$\n",
    "- Common choice is $\\alpha=1$\n",
    "- There is a narrow range of non-linearity, so we can also pick $\\alpha$ such that the inputs fall in the nonlinear range\n",
    "$\\alpha = \\frac{10}{n \\max{|x_i|}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f1916-2f7e-4f41-874c-e1efa490e1fc",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "#### Training\n",
    "- We have $T$ pairs of $(x_k,y_k)$ coordinates that we can use to train our model\n",
    "- Important: remember that our y-values have to be scaled to (0,1), so they are in the same range that our function $g(p)$ maps to\n",
    "- Recall that $g$ is a scalar function:\n",
    "  $g(\\mathbb{A}\\cdot \\vec{x}) = \\vec{y}$\n",
    "  $\\implies z_i = g([\\mathbb{A}\\cdot\\vec{x}]_i) = g\\left( \\sum_j A_{ij} x_j \\right)$\n",
    "\n",
    "- We seek the elements of $\\mathbb{A}$\n",
    "- This can be expressed as a minimization problem, where we alter the matrix elements to achieve this agreement:\n",
    "- Defining the loss function\n",
    "$f(A_{ij}) = \\left| g(\\mathbb{A} x^k) - y^k \\right|^2$\n",
    "- The learning is then perscribed by:\n",
    "$\\vec{x} \\leftarrow x - \\eta \\nabla u$\n",
    "\n",
    "We have the following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e3137-5ddf-4746-92d0-547811a456f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of steepest (gradient) descent minimization\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rosenbrock(x0, x1, a, b):\n",
    "    return (a - x0)**2 + b*(x1 - x0**2)**2\n",
    "\n",
    "def drosdx(x, a, b):\n",
    "    x0 = x[0]\n",
    "    x1 = x[1]\n",
    "    return np.array([-2.0*(a - x0) - 4.0*b*x0*(x1 - x0**2),\n",
    "                     2.0*b*(x1 - x0**2)])\n",
    "\n",
    "def inside(x, bounds):\n",
    "    return ((x > bounds[:,0]) & (x < bounds[:,1])).all()\n",
    "\n",
    "def main():\n",
    "\n",
    "    xmin = -2.0\n",
    "    xmax = 2.0\n",
    "    ymin = -1.0\n",
    "    ymax = 3.0\n",
    "\n",
    "    a = 1.0\n",
    "    b = 100.0\n",
    "\n",
    "    N = 256\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "    y = np.linspace(ymin, ymax, N)\n",
    "\n",
    "    x2d, y2d = np.meshgrid(x, y, indexing=\"ij\")\n",
    "\n",
    "    plt.imshow(np.log10(np.transpose(rosenbrock(x2d, y2d, a, b))), \n",
    "               origin=\"lower\",\n",
    "               extent=[xmin, xmax, ymin, ymax])\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"min_2d_start.png\", dpi=150)\n",
    "\n",
    "\n",
    "    # do descent\n",
    "    xp = np.array([-1.0, 1.5])\n",
    "    xp_old = 1000*xp\n",
    "\n",
    "    eps = 1.e-5\n",
    "\n",
    "    eta = 0.002\n",
    "\n",
    "\n",
    "    \n",
    "    iter = 0\n",
    "    bounds = np.array([[xmin, xmax], [ymin, ymax]])\n",
    "    while np.linalg.norm(xp - xp_old) > eps and inside(xp, bounds):\n",
    "        xp_old[:] = xp[:]\n",
    "\n",
    "        ### In-class problem: Complete your implementation of steepest descent here\n",
    "        grad_ = drosdx(xp_old, a, b)\n",
    "        xp = xp_old - eta * grad_\n",
    "\n",
    "        iter += 1\n",
    "        if iter % 500 == 0:\n",
    "            print(f\"Iteration {iter}: Completed\")\n",
    "    \n",
    "        plt.plot([xp_old[0], xp[0]], [xp_old[1], xp[1]], color=\"C1\")\n",
    "\n",
    "    plt.scatter([xp[0]], [xp[1]], marker=\"o\", color=\"C1\")    \n",
    "    if not inside(xp, bounds):\n",
    "        print(f\"Exited because point {xp} exited the allocated bounds {bounds}\")\n",
    "        plt.title(\"Failed\")\n",
    "    else:\n",
    "        print(f\"Found minimum {xp} in {iter} iterations\")\n",
    "        plt.title(\"Success\")\n",
    "\n",
    "    plt.savefig(f\"min_2d_descent_eta-{eta}.png\", dpi=150)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aba679-ba37-4d72-9eba-9dde03d05e7a",
   "metadata": {},
   "source": [
    "#### Caveats\n",
    "- When you minimize with one set of training data, there is no guarantee that you are still minimized with respect to the previous sets\n",
    "- In practice, you feed the training data multiple times, in random order to the minimizer\n",
    "- Each pass is called an epoch\n",
    "\n",
    "#### Neural Net Minimization\n",
    "We are minimizing the function\n",
    "$$ f(A_{ij}) = |g(Ax^k) - y^k|^2 $$\n",
    "such that\n",
    "$$ (x^k,y^k) = ([x_1^k, x_2^k, \\dots, x_n^k], [y_1^k, y_2^k, \\dots, y_n^k]) \\qquad .$$\n",
    "Each update would be:\n",
    "$$ A^{new}_{pq} = A_{pq} - \\eta \\frac{\\partial f}{\\partial A_{pq} } $$\n",
    "so\n",
    "$$ f(A_{ij}) = \\sum_{i=1}^m \\left [ g\\left(\\sum_{j=1}^n A_{ij} x_j \\right ) - y_i\\right ]^2 $$\n",
    "\n",
    "The derivative $\\frac{\\partial f}{\\partial A_{ij}}$ is given by \n",
    "$$\\frac{\\partial f}{\\partial A_{ij}} = 2(z_p - y_p) \\alpha z_p (1-z_p)x_q$$\n",
    "where $\\vec{z} = \\mathbb{A}\\cdot\\vec{x}$ and $\\alpha$ is the sigmoid parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08523c-9b1c-41f1-9977-0f400024ef76",
   "metadata": {},
   "source": [
    "## Hidden Layers\n",
    "\n",
    "The first and last layer are the input and output layers, respectively.\\\n",
    "Any intermediate steps are denoted _Hidden Layers_ in the neural net. Any neural net which has a hidden layer is a _deep_ network.\\\n",
    "Consider a network which has a single hidden layer, the transformation described by the matrix $\\mathbb{B}$.\\\n",
    "Then, minimization of the (here, $L^2$) error is done in a backwards order, in practice: this is known as _back-propagation_. The error is given by \n",
    "$$ E = \\sum (z - y)^2 $$\n",
    "where $z$ is the prediction and $y$ is the true value.\\\n",
    "The backpropagation technique using gradient descent performs the gradient descent algorithm on $\\mathbb{A}$ and $\\mathbb{B}$ _separately_. Output error backpropagates to the intermediate, hidden layer, and the hidden layer error backpropagates to the input.\n",
    "\n",
    "#### In practice\n",
    "- usually only a single hidden layer is needed\n",
    "- the number of nodes in each layer is also not normally fixed\n",
    "- it is common practice to \"down-select\" to only the most important variables in the hidden layer\n",
    "- then, usually only a single variable or few sets of variables is necessary to output\n",
    "- See tensorflow demo online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8228f-6a78-4d90-862f-a402363376f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
